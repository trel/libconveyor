#include "libconveyor/conveyor.h"
#include <vector>
#include <mutex>
#include <condition_variable>
#include <thread>
#include <atomic>
#include <cerrno>
#include <algorithm>
#include <chrono>
#include <queue> // For std::queue

#ifndef O_ACCMODE
#define O_ACCMODE (O_RDONLY | O_WRONLY | O_RDWR)
#endif

namespace libconveyor {

struct RingBuffer { // Still needed for read buffer
    std::vector<char> buffer;
    size_t capacity = 0;
    size_t head = 0;
    size_t tail = 0;
    size_t size = 0;

    RingBuffer(size_t cap) : capacity(cap), buffer(cap) {}

    size_t write(const char* data, size_t len) {
        if (len == 0) return 0;
        size_t bytes_to_write = std::min(len, capacity - size);
        if (bytes_to_write == 0) return 0;
        size_t first_chunk_len = std::min(bytes_to_write, capacity - head);
        std::memcpy(buffer.data() + head, data, first_chunk_len);
        if (bytes_to_write > first_chunk_len) {
            std::memcpy(buffer.data(), data + first_chunk_len, bytes_to_write - first_chunk_len);
        }
        head = (head + bytes_to_write) % capacity;
        size += bytes_to_write;
        return bytes_to_write;
    }

    size_t read(char* data, size_t len) {
        if (len == 0) return 0;
        size_t bytes_to_read = std::min(len, size);
        if (bytes_to_read == 0) return 0;
        size_t first_chunk_len = std::min(bytes_to_read, capacity - tail);
        std::memcpy(data, buffer.data() + tail, first_chunk_len);
        if (bytes_to_read > first_chunk_len) {
            std::memcpy(data + first_chunk_len, buffer.data(), bytes_to_read - first_chunk_len);
        }
        tail = (tail + bytes_to_read) % capacity;
        size -= bytes_to_read;
        return bytes_to_read;
    }

    void clear() { size = 0; head = 0; tail = 0; }
    bool empty() const { return size == 0; }
    bool full() const { return size == capacity; }
    size_t available_space() const { return capacity - size; }
    size_t available_data() const { return size; }
};

struct WriteRequest {
    std::vector<char> data;
    off_t offset;
};

struct ConveyorImpl {
    storage_handle_t handle;
    int flags;
    storage_operations_t ops;

    bool write_buffer_enabled = false;
    std::queue<WriteRequest> write_queue; // Replaced RingBuffer with std::queue
    size_t write_buffer_capacity; // To enforce limits on write_queue size
    std::atomic<size_t> write_queue_bytes{0}; // Track total bytes in queue
    std::thread write_worker_thread;
    std::mutex write_mutex;
    std::condition_variable write_cv_producer;
    std::condition_variable write_cv_consumer;
    std::atomic<bool> write_worker_stop_flag{false};
    std::atomic<bool> write_buffer_needs_flush{false};
    
    bool read_buffer_enabled = false;
    RingBuffer read_buffer;
    std::thread read_worker_thread;
    std::mutex read_mutex;
    std::condition_variable read_cv_producer;
    std::condition_variable read_cv_consumer;
    std::atomic<bool> read_worker_stop_flag{false};
    std::atomic<bool> read_buffer_stale{false};
    std::atomic<bool> read_worker_needs_fill{false};
    std::atomic<bool> read_eof_flag{false};

    std::atomic<off_t> current_pos_in_storage{0};
    off_t current_file_offset = 0;

    struct Stats {
        std::atomic<size_t> bytes_written{0};
        std::atomic<size_t> bytes_read{0};
        std::atomic<size_t> total_write_latency_ms{0};
        std::atomic<size_t> write_ops_count{0};
        std::atomic<size_t> total_read_latency_ms{0};
        std::atomic<size_t> read_ops_count{0};
        std::atomic<size_t> write_buffer_full_events{0};
        std::atomic<int> last_error_code{0};
    } stats;
    
    ConveyorImpl(size_t w_cap, size_t r_cap) 
        : write_buffer_capacity(w_cap), // Store capacity for queue
          read_buffer(r_cap) {}

    void writeWorker() {
        while (true) {
            std::unique_lock<std::mutex> lock(write_mutex);
            write_cv_consumer.wait(lock, [&] { 
                return !write_queue.empty() || write_buffer_needs_flush || write_worker_stop_flag; 
            });
            if (write_worker_stop_flag && write_queue.empty()) break;

            if (write_queue.empty()) { // Only means a flush was requested but queue is empty
                write_buffer_needs_flush = false;
                write_cv_producer.notify_all(); // Unblock anyone waiting for flush
                continue;
            }

            WriteRequest req = write_queue.front();
            write_queue.pop();
            write_queue_bytes -= req.data.size(); // Decrement bytes count
            
            write_cv_producer.notify_all(); // Notify producers that space is available

            lock.unlock(); // Unlock before performing I/O

            // Handle O_APPEND semantics here, before pwrite
            off_t write_pos = req.offset;
            if (flags & O_APPEND) {
                // For append mode, seek to end *just before* writing to ensure data is appended
                // It's important to use the underlying lseek directly here.
                write_pos = ops.lseek(handle, 0, SEEK_END); 
                if (write_pos == LIBCONVEYOR_ERROR) {
                    lock.lock(); // Re-lock to update stats.last_error_code
                    if (stats.last_error_code.load() == 0) stats.last_error_code = errno;
                    lock.unlock();
                    continue; // Skip this write and continue loop
                }
            }
            
            auto start = std::chrono::steady_clock::now();
            ssize_t written_bytes = ops.pwrite(handle, req.data.data(), req.data.size(), write_pos);
            auto end = std::chrono::steady_clock::now();
            
            lock.lock(); // Re-lock to update shared state
            stats.total_write_latency_ms += std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count();
            stats.write_ops_count++;
            
            if (written_bytes < 0) {
                if (stats.last_error_code.load() == 0) stats.last_error_code = errno;
            } else if (written_bytes > 0) {
                current_pos_in_storage = write_pos + written_bytes;
            }
        }
    }

    void readWorker() {
        std::vector<char> temp_buffer;
        temp_buffer.reserve(read_buffer.capacity);
        while (true) {
            std::unique_lock<std::mutex> lock(read_mutex);
            read_cv_producer.wait(lock, [&] { 
                return read_buffer.available_space() > 0 || read_buffer_stale.load() || read_worker_stop_flag.load() || read_worker_needs_fill.load(); 
            });
            if (read_worker_stop_flag.load()) break;
            if (read_buffer_stale.load()) { read_buffer.clear(); read_buffer_stale = false; }

            if (read_buffer.available_space() > 0 && !read_worker_stop_flag.load()) {
                off_t read_pos = current_file_offset + read_buffer.available_data();
                size_t n = read_buffer.available_space();
                temp_buffer.resize(n);
                lock.unlock(); // Unlock before performing I/O

                auto start = std::chrono::steady_clock::now();
                ssize_t bytes_read = ops.pread(handle, temp_buffer.data(), n, read_pos);
                auto end = std::chrono::steady_clock::now();
                
                lock.lock(); // Re-lock to update shared state
                stats.total_read_latency_ms += std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count();
                stats.read_ops_count++;

                if (bytes_read > 0) {
                    read_buffer.write(temp_buffer.data(), bytes_read);
                } else if (bytes_read == 0) {
                    read_eof_flag = true;
                } else if (stats.last_error_code.load() == 0) {
                    stats.last_error_code = errno;
                }
            }
            if (read_worker_needs_fill.load()) read_worker_needs_fill = false;
            read_cv_consumer.notify_all();
        }
    }
};
}

conveyor_t* conveyor_create(storage_handle_t h, int f, const storage_operations_t& o, size_t w, size_t r) {
    auto* impl = new libconveyor::ConveyorImpl(w, r);
    impl->handle = h; impl->flags = f; impl->ops = o;
    int mode = f & O_ACCMODE;
    impl->read_buffer_enabled = (mode == O_RDONLY || mode == O_RDWR);
    impl->write_buffer_enabled = (mode == O_WRONLY || mode == O_RDWR);
    if (impl->read_buffer_enabled) impl->read_worker_thread = std::thread(&libconveyor::ConveyorImpl::readWorker, impl);
    if (impl->write_buffer_enabled) impl->write_worker_thread = std::thread(&libconveyor::ConveyorImpl::writeWorker, impl);
    return reinterpret_cast<conveyor_t*>(impl);
}

void conveyor_destroy(conveyor_t* conv) {
    if (!conv) return;
    auto* impl = reinterpret_cast<libconveyor::ConveyorImpl*>(conv);
    if (impl->write_buffer_enabled) conveyor_flush(conv);
    if (impl->read_buffer_enabled) {
        impl->read_worker_stop_flag = true;
        impl->read_cv_producer.notify_all();
        impl->read_cv_consumer.notify_all();
        if (impl->read_worker_thread.joinable()) impl->read_worker_thread.join();
    }
    if (impl->write_buffer_enabled) {
        impl->write_worker_stop_flag = true;
        impl->write_cv_producer.notify_all();
        impl->write_cv_consumer.notify_all();
        if (impl->write_worker_thread.joinable()) impl->write_worker_thread.join();
    }
    delete impl;
}

ssize_t conveyor_write(conveyor_t* conv, const void* buf, size_t count) {
    if (!conv) { errno = EBADF; return LIBCONVEYOR_ERROR; }
    auto* impl = reinterpret_cast<libconveyor::ConveyorImpl*>(conv);
    if (!impl->write_buffer_enabled) { errno = EBADF; return LIBCONVEYOR_ERROR; }
    if (impl->stats.last_error_code.load() != 0) { errno = impl->stats.last_error_code.load(); return LIBCONVEYOR_ERROR; }

    if (count > impl->write_buffer_capacity) {
        errno = EMSGSIZE;
        return LIBCONVEYOR_ERROR;
    }
    
    std::unique_lock<std::mutex> lock(impl->write_mutex);
    
    if(!impl->write_cv_producer.wait_for(lock, std::chrono::seconds(30), [&]{ 
        return (impl->write_queue_bytes.load() + count <= impl->write_buffer_capacity) || impl->write_worker_stop_flag; 
    })) {
        errno = ETIMEDOUT;
        return LIBCONVEYOR_ERROR;
    }

    if (impl->write_worker_stop_flag) return LIBCONVEYOR_ERROR;

    libconveyor::WriteRequest req;
    req.data.assign(static_cast<const char*>(buf), static_cast<const char*>(buf) + count);
    req.offset = impl->current_file_offset;
    impl->write_queue.push(req);
    impl->write_queue_bytes += count;
    
    impl->current_file_offset += count;
    impl->stats.bytes_written += count;

    impl->write_cv_consumer.notify_one();

    return count;
}

ssize_t conveyor_read(conveyor_t* conv, void* buf, size_t count) {
    if (!conv) { errno = EBADF; return LIBCONVEYOR_ERROR; }
    auto* impl = reinterpret_cast<libconveyor::ConveyorImpl*>(conv);
    if (!impl->read_buffer_enabled) { errno = EBADF; return LIBCONVEYOR_ERROR; }
    if (impl->stats.last_error_code.load() != 0) { errno = impl->stats.last_error_code.load(); return LIBCONVEYOR_ERROR; }
    std::unique_lock<std::mutex> lock(impl->read_mutex);
    ssize_t total_read = 0;
    char* ptr = static_cast<char*>(buf);
    while (total_read < count && !impl->read_worker_stop_flag.load()) {
        if (impl->read_buffer.empty()) {
            if (impl->read_eof_flag.load()) break;
            impl->read_worker_needs_fill = true;
            impl->read_cv_producer.notify_one();
            impl->read_cv_consumer.wait(lock, [&]{ return impl->read_buffer.available_data() > 0 || impl->read_eof_flag.load() || impl->read_worker_stop_flag.load(); });
            if (impl->read_buffer.available_data() == 0) break;
        }
        size_t read_now = impl->read_buffer.read(ptr + total_read, count - total_read);
        total_read += read_now;
        impl->current_file_offset += read_now;
        impl->read_cv_producer.notify_one();
    }
    impl->stats.bytes_read += total_read;
    return total_read;
}

off_t conveyor_lseek(conveyor_t* conv, off_t offset, int whence) {
    if (!conv) { errno = EBADF; return LIBCONVEYOR_ERROR; }
    auto* impl = reinterpret_cast<libconveyor::ConveyorImpl*>(conv);
    
    std::unique_lock<std::mutex> read_lock(impl->read_mutex, std::defer_lock);
    std::unique_lock<std::mutex> write_lock(impl->write_mutex, std::defer_lock);
    std::lock(read_lock, write_lock);

    if (impl->write_buffer_enabled && !impl->write_queue.empty()) {
        impl->write_buffer_needs_flush = true;
        impl->write_cv_consumer.notify_one();
        impl->write_cv_producer.wait(write_lock, [&] { return impl->write_queue.empty() || impl->write_worker_stop_flag; });
        impl->write_buffer_needs_flush = false;
    }

    off_t new_pos = impl->ops.lseek(impl->handle, offset, whence);

    if (new_pos != LIBCONVEYOR_ERROR) {
        if (impl->read_buffer_enabled) {
            impl->read_buffer.clear();
            impl->read_buffer_stale = true;
            impl->read_eof_flag = false;
            impl->read_cv_consumer.notify_all();
            impl->read_cv_producer.notify_all();
        }
        impl->current_file_offset = new_pos;
        impl->current_pos_in_storage = new_pos;
    }
    return new_pos;
}

int conveyor_flush(conveyor_t* conv) {
    if (!conv) { errno = EBADF; return LIBCONVEYOR_ERROR; }
    auto* impl = reinterpret_cast<libconveyor::ConveyorImpl*>(conv);
    if (!impl->write_buffer_enabled) return 0;
    if (impl->stats.last_error_code.load() != 0) { errno = impl->stats.last_error_code.load(); return LIBCONVEYOR_ERROR; }
    std::unique_lock<std::mutex> lock(impl->write_mutex);
    if (!impl->write_queue.empty()) {
        impl->write_buffer_needs_flush = true;
        impl->write_cv_consumer.notify_one();
        impl->write_cv_producer.wait(lock, [&] { return impl->write_queue.empty() || impl->write_worker_stop_flag; });
    }
    impl->write_buffer_needs_flush = false;
    if (impl->stats.last_error_code.load() != 0) { errno = impl->stats.last_error_code.load(); return LIBCONVEYOR_ERROR; }
    return 0;
}

int conveyor_get_stats(conveyor_t* conv, conveyor_stats_t* stats) {
    if (!conv || !stats) {
        errno = EBADF;
        return LIBCONVEYOR_ERROR;
    }
    auto* impl = reinterpret_cast<libconveyor::ConveyorImpl*>(conv);
    
    stats->bytes_written = impl->stats.bytes_written.exchange(0);
    stats->bytes_read = impl->stats.bytes_read.exchange(0);
    size_t w_latency = impl->stats.total_write_latency_ms.exchange(0);
    size_t w_ops = impl->stats.write_ops_count.exchange(0);
    size_t r_latency = impl->stats.total_read_latency_ms.exchange(0);
    size_t r_ops = impl->stats.read_ops_count.exchange(0);
    stats->write_buffer_full_events = impl->stats.write_buffer_full_events.exchange(0);
    stats->last_error_code = impl->stats.last_error_code.load();
    stats->avg_write_latency_ms = (w_ops > 0) ? (w_latency / w_ops) : 0;
    stats->avg_read_latency_ms = (r_ops > 0) ? (r_latency / r_ops) : 0;

    return 0;
}